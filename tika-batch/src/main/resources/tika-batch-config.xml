<?xml version="1.0" encoding="UTF-8" standalone="no" ?>

<tika-batch-config staleThresholdMillis="300000" maxQueueSize="10000" numConsumers="5">

	<crawler class="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
		randomCrawl="true" 
		maxFilesToAdd="-1" 
		maxFilesToConsider="-1" 
		includeFilePat="(?i).pdf$"
		excludeFilePat="(?i).msg$"
		maxFileSizeBytes="-1"
        srcDir="input"
        />
<!--
    This is an example of a crawler that reads a list of files to be processed from a
    file.  This assumes that the files in the list are relative to srcDir.
    <crawler class="org.apache.tika.batch.fs.builders.FSCrawlerBuilder"
             fileList="files.txt"
             fileListEncoding="UTF-8"
             maxFilesToAdd="-1"
             maxFilesToConsider="-1"
             includeFilePat="(?i).pdf$"
             excludeFilePat="(?i).msg$"
             maxFileSizeBytes="-1"
             srcDir="input"
    />
-->
	<consumers class="org.apache.tika.batch.fs.builders.BasicTikaFSConsumersBuilder">
		<parser class="AutoDetectParserFactory" parseRecursively="true"/>
		<contenthandler class="org.apache.tika.batch.builders.DefaultContentHandlerFactoryBuilder"
                        basicHandlerType="xml" writeLimit="-1"/>
		
		<!-- overwritePolicy: "skip" a file if target file exists, "rename" a target file, "overwrite" -->
		<outputstream class="FSOutputStreamFactory" overwritePolicy="rename" targDir="output"
                encoding="UTF-8"/>
	</consumers>
	
	<!-- reporter and interrupter are optional -->
	<reporter class="StdOutStatusReporterFactory"/>
	<interrupter class="Interrupter"/>
</tika-batch-config>